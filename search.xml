[{"title":"MLOps","url":"https://wj-mcat.github.io/2021/05/11/nlp/MLOps/","content":"","categories":["nlp"],"tags":[]},{"title":"干货铺","url":"https://wj-mcat.github.io/2021/05/08/overall/","content":"<p>收藏干货</p>\n<span id=\"more\"></span>\n<h1>NLP</h1>\n<h2 id=\"Papers\">Papers</h2>\n<ul>\n<li><a href=\"https://github.com/km1994/nlp_paper_study\">nlp-paper-study</a></li>\n<li><a href=\"https://docs.google.com/spreadsheets/d/1YfuUjrqCGLW0aq2l8QsCIOS0N1fwhtrTvyGI5EqUkMs/edit#gid=0\">few-shot papers</a></li>\n<li><a href=\"https://github.com/Duan-JM/awesome-papers-fewshot\">awesome-papers-fewshot</a></li>\n<li><a href=\"https://github.com/yizhen20133868/Awesome-SLU-Survey#few-shot-slu\">Awesome-SLU-Survey</a></li>\n</ul>\n","categories":[],"tags":[]},{"title":"Attention","url":"https://wj-mcat.github.io/2021/05/08/nlp/components/attention/","content":"<p>Attention在实际项目中的应用非常广泛。</p>\n<span id=\"more\"></span>\n","categories":["nlp","components"],"tags":["nlp"]},{"title":"few-shot papers","url":"https://wj-mcat.github.io/2021/05/07/nlp/papers/few-shot/","content":"<p>Few-Shot 在工业界落地非常吃香，很大原因在于让客户给你标注数据的代价太大，就算给你标，也很容易出现脏数据，此时few-shot就显得尤为重要。</p>\n<span id=\"more\"></span>\n<h1>介绍</h1>\n<h2 id=\"什么是Few-Shot\">什么是Few-Shot</h2>\n<p>Few-Shot learning(FSL)，在少量数据集（通常2个，5个左右）上进行预测的深度学习方法。</p>\n<p>目前的机器学习中，都是基于大量数据来训练模型，甚至流行一句话：数据多又好，模型才能好。可这个在现实工业落地场景中是一个不实际的场景，更多的都是少又脏的数据，而Few-Shot学习则主要是为了解决这样的问题。</p>\n<p>为什么few-shot能work且这么重要呢？</p>\n<ul>\n<li>像人类一样判断：few-shot 通过把query和support set进行对比，找到相似度最高的那个类别，这个过程和人类来判断一个新事物的方式是非常类似的，也容易理解。</li>\n<li>在少量数据集上学习：通过将query与提供的少量支撑集进行相似度对比计算，得到query的类别，这个过程能够基于非常少量的数据集上学习到一个很好的效果，这个对于实际工业落地来说，非常的重要。</li>\n<li>降低数据搜集和算力的消耗：由于数据集非常少，搜集起来就相对简单，同时计算也非常快，这也是对于工业界非常有益。</li>\n</ul>\n<h2 id=\"few-shot-VS-zero-shot\">few-shot VS zero-shot</h2>\n<h2 id=\"方法\">方法</h2>\n<ul>\n<li>Pretraing：基于某一个预训练模型来对输入进行编码，提取特征。</li>\n<li>fine-tuning: SoftMax Linear\n<ul>\n<li>用supoort set的向量平均数据作为初始值</li>\n<li>entropy regularization</li>\n<li>cosine similarity + Softmax Linear Classifier</li>\n</ul>\n</li>\n<li>few-shot prediction：根据一个query在support set中进行预测，一类support set中的特征将会取平均。</li>\n</ul>\n<h3 id=\"Entropy-Regularization\">Entropy Regularization</h3>\n<p>属于强化学习中的一个正则化方法。</p>\n<p>在正式讲解之前，需要预习一下Entropy基本概念：如果类别判断的信心分数很低（类别分数平均），则Entropy score很高；如果类别判断的信心分数很高（类别分数不平均，可以肯定是属于哪类），则Entropy score很低。</p>\n<blockquote>\n<p>信息熵本身就是描述一件事物的混乱程度，且始终趋于混乱程度更高的方向发展。</p>\n</blockquote>\n<p>作用：</p>\n<ul>\n<li>Improve Exploration</li>\n</ul>\n<p>提升模型的优化能力，避免local optimization。</p>\n<ul>\n<li>Fine-Tuning Policies</li>\n</ul>\n<p>在Fine-Tuning过程中，使用Entropy Regularization能够让模型优化更加具有探索性，避免过拟合，让模型总是有探索性的去优化全局参数。</p>\n<ul>\n<li>More Robustness</li>\n</ul>\n<p>由于模型在fine-tuning阶段更具有探索性，故模型能够避免局部优化，提升全局优化，提升模型的鲁棒性。</p>\n<blockquote>\n<p>参考论文：<a href=\"https://www.aclweb.org/anthology/2020.acl-main.615.pdf\">Generalized Entropy Regularization or: There’s Nothing Special about Label Smoothing</a></p>\n</blockquote>\n<h2 id=\"数据集\">数据集</h2>\n<ul>\n<li>Omnilglot：手下数字识别，类别很多，数据集非常小，很适合在学术界上学习测试</li>\n<li>Mini-ImageNet：100classes</li>\n</ul>\n<h2 id=\"孪生网络\">孪生网络</h2>\n<blockquote>\n<p>连体网络</p>\n</blockquote>\n<p>Encoder层级是一致的。</p>\n<p>训练数据是需要进行正负例样本。<br>\nLoss： Triple Loss</p>\n<p>One-Shot Prediction：最后只需要计算distance</p>\n<p>步骤：</p>\n<ul>\n<li>使用孪生网络在是大型数据集上做训练</li>\n<li>构造support-set\n<ul>\n<li>训练数据集是包含测试集中的类别</li>\n</ul>\n</li>\n<li>使用孪生网络来计算其相似度</li>\n</ul>\n<h2 id=\"技巧\">技巧</h2>\n<h1>论文列表</h1>\n<h2 id=\"2021\">2021</h2>\n<h3 id=\"PROTODA-EFFICIENT-TRANSFER-LEARNING-FOR-FEW-SHOT-INTENT-CLASSIFICATION\">PROTODA: EFFICIENT TRANSFER LEARNING FOR FEW-SHOT INTENT CLASSIFICATION</h3>\n<hr>\n<p>参考链接：</p>\n<ul>\n<li><a href=\"https://blog.bitsrc.io/how-to-write-beautiful-and-meaningful-readme-md-for-your-next-project-897045e3f991\">How to Write Beautiful and Meaningful README.md</a></li>\n<li><a href=\"https://arxiv.org/abs/1904.04232\">A Closer Look at Few-shot Classification</a></li>\n<li><a href=\"https://arxiv.org/abs/1909.02729\">A Baseline for few-shot Image classification</a></li>\n<li><a href=\"https://arxiv.org/abs/2003.04390\">A New Meta-Baseline for Few-Shot Learning</a></li>\n</ul>\n","categories":["nlp","papers"],"tags":["nlp"]},{"title":"如何编写一个好的README","url":"https://wj-mcat.github.io/2021/05/01/skills/how-to-write-readme/","content":"<p>作为一个优秀的开源项目推广者。</p>\n<span id=\"more\"></span>\n<hr>\n<p>参考链接：</p>\n<ul>\n<li><a href=\"https://blog.bitsrc.io/how-to-write-beautiful-and-meaningful-readme-md-for-your-next-project-897045e3f991\">How to Write Beautiful and Meaningful README.md</a></li>\n</ul>\n","categories":["skills"],"tags":[]},{"title":"Technical Writer","url":"https://wj-mcat.github.io/2021/05/01/skills/technical-writer/","content":"<p>俗称，技术写手；融合写作技巧和沟通技巧，将复杂技术问题逐步拆解成容易理解的系列文档，这在企业级互联网公司中显得尤为重要，一个好的技术文档系统，能够帮助客户更快理解业务，上手工具使用，从而提升产品整体使用体验。</p>\n<p>对于热爱分享的Programmer来说，这是散播影响力的重要方式，所以学习如何做一个好的<code>Technical Writer</code>对于技术人员生涯来说，是一个非常不错的选择。</p>\n<span id=\"more\"></span>\n<p>在这篇文章中，我将大家介绍：什么是Technical Writer？如何成为一个Technical Writer？</p>\n<hr>\n<h2 id=\"什么是Technical-Writer\">什么是Technical Writer</h2>\n<h3 id=\"概念\">概念</h3>\n<p>Technical Writer 是一个专业的活，负责简化复杂的技术系统，所以具体要做什么呢？</p>\n<ul>\n<li>系统介绍手册：比如：Rasa</li>\n<li>用户手册，比如：开源项目技术文档</li>\n<li>期刊文章，比如：知乎博文</li>\n<li>快速参考指南，比如：Quick Start</li>\n<li>白皮书，这个就比较复杂</li>\n</ul>\n<p>对于技术人员来说，会为开源项目编写用户手册，期刊文章等，放开发者能够从多方面了解该项目的细节，从而快速上手，提高该项目的知名度（例如：Github star），此时会给自己一个正向的反馈，从而更有信心来继续完善该项目。这个也是快速积攒人气，宣传自己的方式。</p>\n<h3 id=\"Technical-Writer-的目的\">Technical Writer 的目的</h3>\n<p>相信所有的技术人员都想成为大佬，这个必须依赖你的能力。尤雨溪在<a href=\"https://www.zhihu.com/question/456527668/answer/1858291784\">《怎么才能有尤雨溪一半强，该怎么学习？》</a>中的回答提到，其实强并没有很明确的分界线，可是在大众眼光中看来，强可由在影响力体现，而编写优秀的技术博客就是一个产生影响力的方式。</p>\n<p>不过回过头来，我们并不能抱着产生影响力的初衷来写技术博客，而是抱着钻研技术的态度来编写每一篇博客，这样才能让自己对每一个问题都有独特的见解，从而逐步提升自己的能力。</p>\n<h2 id=\"一定要成为Technical-Writer\">一定要成为Technical Writer</h2>\n<h3 id=\"最好的学习方法\">最好的学习方法</h3>\n<p>相信很多技术人员都在感叹，日新月异的技术根本学不完，这个月出的技术下个月就被颠覆了，如果过了三十该何去何从。可这是由于对技术不热爱，技术也是艺术的一种，而且这种艺术还能够改变世界，是一件非常酷的事情。纵观国外的技术氛围，有很多白发苍苍的技术大佬在各种论坛上分享这段时间工作的成果，我们也都在钦佩他们的之外在感叹30岁的危机。</p>\n<p>发布高质量博客获得知名度，而知名度是继续学习发布博客最大的动力，良心循环是学习最好的现象。如果你是一个技术人员，快去尝试通过博客提升技术底蕴和知名度吧，相信你不会后悔。</p>\n<h3 id=\"行动力\">行动力</h3>\n<p>有很多小伙伴，一开始说要学着写博客，可是很多都会卡在博客的搭建这个环节。会纠结到底是发布到哪个平台上比较合适，用什么主题比较合适，写博客的时间规划是多久比价合适等一系列的问题，曾经的我也被这些问题所困扰，迟迟没有开始。或许这都是需要有一个过程吧。</p>\n<p>当你想要实现某个想法的时候，此时此刻是最好的开始，一定不要delay到某个特殊时期，大数据告诉我们，这很可能会夭折，行动力就是当你有某个想法时，just do it.</p>\n<h3 id=\"可能性\">可能性</h3>\n<p>技术人员只做技术并不是一个好的方向，因为这样以后就只能做技术。技术人员以后可以有很多发展方向，比如技术管理层、架构师、咨询师以及资深技术人员，甚至在生涯当中有机会编写一本技术书籍，而这些方向一切的基础都是拥有足够的技术底蕴以及了解新技术现状。</p>\n<p>而技术博客能够让技术人员探索当前热门技术的底层原理，结合自己的经验来剖析技术原理，让开发者能够从多个维度理解相关技术，这些都是能够给自己积攒知名度，对于以后找工作，公司提升基于来说都是一个非常不错的指标。坚持做好技术分享能够拓宽技术路线，给自己多个选择，消解国内的30岁危机。</p>\n<p>所以啊，坚持初衷，坚持写技术分享博客可以拓宽技术人员未来可能性，让未来的我们走的更加悠然。</p>\n<h3 id=\"自制力\">自制力</h3>\n<p>坚持 意见事情真的挺难的，至少对于普通人来说是这个样子。</p>\n<p>相信有很多小伙伴都存在这样的苦恼，曾经制定过无数计划，都被遗忘在某个角落，等尝试过失败后却后悔莫及，为什么当初没有坚持下来。</p>\n<p>我不想再这个样子了，管理好自己的时间，充分利用好一分一秒，做自己不会后悔的事情。我想，这件事情能够坚持下来，那什么事情我不能坚持下来呢？</p>\n<hr>\n<h2 id=\"如何成为一个Technical-Writer？\">如何成为一个Technical Writer？</h2>\n<p>之前跟一个出版过书籍的朋友聊过，如何写技术博客？他只给我说了一句话：先别管写的如何，写出来就是最大的成就。</p>\n<p>所以，要成为一个优秀的Technical Writer，首先第一个就是要成为一个Writer，无论是不是优秀的。这也是为什么，我想要开始写，不想被周边琐碎的事情拖延开始的时间。</p>\n<blockquote>\n<p>写就完事了</p>\n</blockquote>\n<h3 id=\"系统性\">系统性</h3>\n<p>写文章和学习类似，都需要系统性。</p>\n<p>如果你要开始写文章，我相信你肯定是写学习过程中的感悟，技术经验，而这些我相信都是应该高内聚的：在某一个领域内。如果能够在某个领域内坚持写下去，我相信未来的文章肯定会越来愈深入，逐步达到专家级的水平，而这个往往是与自己的实力相匹配。</p>\n<p>学习要有系统性，写文章也需要系统性，让自己专注在某个领域内，不断的探索其中的难题，分享解决方案。</p>\n<h3 id=\"少点拘束\">少点拘束</h3>\n<p>不同阶段拥有不同的表现，在写作上也是如此。</p>\n<p>刚开始写博文，写出来便是最大的进步，其后不断的积累，经验会促使自己调整其结构以及叙述的方式，从而提升技术博文的可阅读性。</p>\n<p>写作是一个螺旋式上升的过程，不要太在意当下写的不好，而是要在意当下哪里写的不好，从而可以调整，这样才能解决主要矛盾，提升自我。</p>\n<h3 id=\"博文不是一蹴而就\">博文不是一蹴而就</h3>\n<p>在工作和学习过程中，对于某个知识点的理解会不断纠错和加深的过程。</p>\n<p>我把博文当成自己的知识库，是一个不断更新的过程。比如我对Bert的理解会随着学习和工作加深对其的理解，于是回头来重新调整内容，加深对应知识点的理解。</p>\n<p>这是一个良性循环的过程，是一个系统性的活儿，是一个长期的活儿，需要坚持下来才会有爆发式的效益增长。</p>\n<h2 id=\"给自己的话\">给自己的话</h2>\n<p>路走了很多遍，很多条，可是我一直在纠结哪一条是最好的。</p>\n<p>其实选择一条路，坚定的走下去，这条路肯定是最好的路。</p>\n","categories":["skills"],"tags":[]},{"title":"模型蒸馏","url":"https://wj-mcat.github.io/2021/04/30/nlp/distillation/","content":"<p>模型蒸馏旨在减模型参数量，减少部署的压力。</p>\n<span id=\"more\"></span>\n<h1>介绍</h1>\n<h2 id=\"名词解释\">名词解释</h2>\n<ul>\n<li>teacher -&gt; 原始模型或模型ensemble</li>\n<li>student -&gt; 蒸馏得到的模型</li>\n<li>transfer set -&gt; 用来迁移teacher知识、训练student的数据集合</li>\n<li>soft target -&gt; teacher输出的预测结果（一般是softmax之后的概率）</li>\n<li>hard target -&gt; 样本原本的标签</li>\n<li>temperature -&gt; 蒸馏目标函数中的超参数</li>\n<li>born-again network -&gt; 蒸馏的一种，指student和teacher的结构和尺寸完全一样</li>\n<li>teacher annealing -&gt; 防止student的表现被teacher限制，在蒸馏时逐渐减少soft targets的权重</li>\n</ul>\n<h2 id=\"\"></h2>\n<hr>\n<p>参考链接：</p>\n<ul>\n<li><a href=\"https://zhuanlan.zhihu.com/p/71986772\">深度神经网络模型蒸馏Distillation</a></li>\n</ul>\n","categories":["nlp"],"tags":["nlp"]},{"title":"pre-commit 工具","url":"https://wj-mcat.github.io/2021/04/29/python/pre-commit/","content":"<p>现在DevOps已然成为项目开发过程中的必备环节，尤其是开源项目，由来自全球的开发者共同维护一个项目，此时严格且规范的代码审查与自动化部署至关重要。而pre-commit工具能够帮助你更好的完成这个工作。</p>\n<p>pre-commit能够在git命令不同阶段添加自定义脚本，从而完成代码审查、日志记录、自动更新版本号以及自动部署文档系统等常见功能，此篇文章将要介绍pre-commit在<a href=\"https://github.com/wechaty/python-wechaty\">python-wechaty</a>开源项目中的实战小技巧。</p>\n<span id=\"more\"></span>\n<h2 id=\"一、快速上手\">一、快速上手</h2>\n<h3 id=\"1-1-安装工具\">1.1 安装工具</h3>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install pre-commit</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>别看是使用python来安装工具，可是在任何编程语言上都是通用的。</p>\n</blockquote>\n<p>安装完成之后即可查看对应工具的版本：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span><span class=\"bash\"> pre-commit --version</span></span><br><span class=\"line\">pre-commit 2.12.1</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-2-安装hook脚本\">1.2 安装hook脚本</h3>\n<p>hook是什么？hook原意钩子，是在git提交之前，提交之后执行的脚本，能够进行代码审查，撤销提交操作等，也是这个工具的核心脚本。</p>\n<p>安装的脚本如下所示：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pre-commit install</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-3-配置文件\">1.3 配置文件</h3>\n<p>当开发者在push代码之前，需要对进行审查，或者push成功之后，在自动更新项目版本文件（VERSION），此类定制化的任务是需要通过配置文件来完成。</p>\n<p>配置文件能够在不同分支和git操作阶段执行自定义任务脚本，完成自定义的代码审查。添加配置文件有多种方式：</p>\n<ul>\n<li>手动添加一个文件：<code>.pre-commit-config.yaml</code></li>\n<li>通过命令行创建一个模板配置文件：<code>pre-commit sample-config</code></li>\n</ul>\n<p>设定一个场景，要在项目本地代码提交之前进行代码审查，其配置文件如下所示：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># the hook execution directory in under git root directory</span></span><br><span class=\"line\"><span class=\"attr\">repos:</span></span><br><span class=\"line\"><span class=\"bullet\">-</span> <span class=\"attr\">repo:</span> <span class=\"string\">local</span></span><br><span class=\"line\">  <span class=\"attr\">hooks:</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"bullet\">-</span> <span class=\"attr\">id:</span> <span class=\"string\">pylint</span></span><br><span class=\"line\">    <span class=\"attr\">name:</span> <span class=\"string\">pylint</span></span><br><span class=\"line\">    <span class=\"attr\">description:</span> <span class=\"string\">&quot;Pylint: Checks for errors in Python code&quot;</span></span><br><span class=\"line\">    <span class=\"attr\">language:</span> <span class=\"string\">python</span></span><br><span class=\"line\">    <span class=\"attr\">entry:</span> <span class=\"string\">pylint</span> <span class=\"string\">./src</span></span><br><span class=\"line\">    <span class=\"attr\">always_run:</span> <span class=\"literal\">true</span></span><br><span class=\"line\">    <span class=\"attr\">verbose:</span> <span class=\"literal\">true</span></span><br><span class=\"line\">    <span class=\"attr\">require_serial:</span> <span class=\"literal\">false</span></span><br><span class=\"line\">    <span class=\"attr\">stages:</span> [<span class=\"string\">push</span>]</span><br><span class=\"line\">    <span class=\"attr\">types:</span> [<span class=\"string\">text</span>]</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-4-参数说明\">1.4 参数说明</h3>\n<ul>\n<li>\n<p>repos<br>\n通过属性名即可得知，此配置文件可对多个repo配置hook，在上述配置当中我只对本地单个repo进行了配置。</p>\n<p>注意，其中的repo名为：local。</p>\n</li>\n<li>\n<p>hooks<br>\n在每个项目当中，可以配置多个hooks任务，从而实现定制化的操作。同样，hooks也是一个列表属性。</p>\n</li>\n<li>\n<p>id/name/description/language/<br>\n此类属性是单个hook任务的描述信息。</p>\n</li>\n<li>\n<p>entry<br>\n这个就是该hook任务的自定义Bash脚本，大家可以在此处编写逻辑较为复杂的代码审查，也可以将复杂的脚本通过外部脚本文件来执行。这部分就是通过程序员自行扩展。</p>\n</li>\n<li>\n<p>verbose<br>\n是否在控制台中打印日志记录，这个通常会设置成True。</p>\n</li>\n<li>\n<p>require_serial<br>\n是否在一个进程当中执行该任务：通常情况下不同任务都是隔离的，没有依赖关系，如果审查的耗时较长，则可以设置为<code>False</code>加快其执行的速度。</p>\n</li>\n<li>\n<p>stages<br>\n当代码在提交时，会触发<code>commit</code>、<code>commit</code>,<code>merge-commit</code>, <code>push</code>, <code>prepare-commit-msg</code>, <code>commit-msg</code>, <code>post-checkout</code>, <code>post-commit</code>, <code>post-merge</code>, or <code>manual</code>，通常情况下使用<code>push</code>即可。</p>\n</li>\n<li>\n<p>types<br>\npre-commit 工具会在不同类型的文件上制定对应的文件，</p>\n</li>\n</ul>\n","categories":["python"],"tags":["python"]},{"title":"关于Bert的一切","url":"https://wj-mcat.github.io/2021/04/28/nlp/pretrained-language-model/bert/","content":"<p>作为NLP领域里程碑式的作品，对于其深刻的理解是很多后续学习工作的基础，更是面试找工作的利器。</p>\n<span id=\"more\"></span>\n<h1>介绍</h1>\n<p><img src=\"/images/2021.05/bert.png\" alt=\"upload successful\"></p>\n<p><code>Bert</code> (<strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers) 是由谷歌AI NLP团队提出：<a href=\"https://arxiv.org/abs/1810.04805\">《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》</a>，是第一个无监督双向语言模型，一经发表就刷新了各大榜单，被定义为里程碑式的模型，让NLP达到了新的高度。</p>\n<p><code>Bert</code>是一个基于海量无结构化文本进行深度双向语义建模的预训练模型，通过<code>Mask Language Model</code>和<code>Next Sentence Prediction</code>两种训练任务，学习到文本中丰富的先验知识。实践证明，这些通用先验知识能够应用于各种下游任务，并取得<code>SOTA</code>效果，即使是在<code>low-resource</code>下也能够取得良好的效果。</p>\n<h2 id=\"解决了什么问题\">解决了什么问题</h2>\n<p>模型参数数量这么大，必然需要海量的训练语料。从哪里收集这些海量的训练语料？《A Neural Probabilistic Language Model》这篇论文说，每一篇文章，天生是训练语料。难道不需要人工标注吗？回答，不需要。</p>\n<h2 id=\"为什么这么强\">为什么这么强</h2>\n<p>原因有三：</p>\n<ul>\n<li>双向建模</li>\n<li>使用MLM和NSP两种训练技巧</li>\n<li>能够学习到Context-Heavy的Representation</li>\n</ul>\n<p>用一句话来说就是：使用MLM和NSP两种训练技巧，基于双向建模能够学习到丰富上下文Representation。</p>\n<h2 id=\"整体结构\">整体结构</h2>\n<p>基于Transformer构建模型，bert-base模型当中参数为：L=12, H=768, A=12，参数数量为110M；Bert-Large模型中参数为：L=24, H=1024, A=16，参数数量为：340M。</p>\n<p><img src=\"/images/2021.05/bert-overall.png\" alt=\"bert-overall\"></p>\n<blockquote>\n<p>Bert其实是分为Encoder和Decoder两个部分，如果只是为了了解Bert是如何工作的，那只需要了解Encoder部分即可。</p>\n</blockquote>\n<p>是由12层的双向Transformer构成，能够进行不同层面的特征抽取，那为了深入到Bert中的细节，那接下来将要介绍一下Bert的细节。</p>\n<h2 id=\"Transformer\">Transformer</h2>\n<p>Transformer是一个基于Attension机制的NLP基础组件，出自于《Attention is All you need》，OpenAI GPT和BERT都是基于Transformer。</p>\n<p>下面我将以数据的流向为线索，来介绍模型中的细节。</p>\n<h3 id=\"问题定义\">问题定义</h3>\n<p>最初Transformer被用来解决翻译的问题，那此时就要求其具备：</p>\n<ul>\n<li>能够步骤输入文本中词与词之间的关系</li>\n<li>能够将以上关系映射到输出文本中</li>\n</ul>\n<p>此时就需要一个Encoder和Decoder来处理这两个问题，结构如下：</p>\n<p><img src=\"/images/2021.05/transformer.png\" alt=\"transformer\"></p>\n<h3 id=\"数据的流向\">数据的流向</h3>\n<p>简化后的结构图如下所示：</p>\n<p><img src=\"/images/2021.05/transformer-data-flow.png\" alt=\"transformer\"></p>\n<blockquote>\n<p>左边Encoder的输入为input sentence，右边Decoder的输入为target sentence，这两者用来做相关相似度计算。</p>\n</blockquote>\n<ol>\n<li>在原始论文中，N为6.</li>\n<li>input sentence每一个单词为token，所以一个输入文本可编码为：<code>input_length</code> * <code>embedding_dim</code></li>\n<li>添加位置信息(positional information)，维度大小同上：<code>input_length</code> * <code>embedding_dim</code></li>\n<li>数据经过N个Encoder之后的输出为：<code>input_length</code> * <code>embedding_dim</code>，数据的原始大小并没有变化</li>\n<li>target sentence进行分词和添加位置信息之后，便输入到Decoder当中，大小为：<code>target_length</code> * <code>embedding_dim</code></li>\n<li>Decoder中的每一层都要使用Encoder的输入，整体输出大小为：<code>target_length</code> * <code>embedding_dim</code></li>\n<li>最后连接一个Linear层，然后其输出大小为：<code>target_length</code> * <code>vocab_size</code></li>\n</ol>\n<p>以上便是Transformer中的数据流向过程，那接下来我将介绍每一个细节。</p>\n<h3 id=\"输入表示\">输入表示</h3>\n<p>input sentence 和 target sentence 都是需要进行编码，最终得到一个<code>input_length</code> * <code>embedding_dim</code>向量，那这是如何得到文本的向量表示的呢？</p>\n<p>主要分为两个步骤：</p>\n<ul>\n<li>Token Embedding</li>\n<li>Encoding of Positions</li>\n</ul>\n<p>原始文本和目标文本的编码过程是一致的。Token Embedding的原理我就不介绍了，与通用的方法一致，Encoding of Positions的原理可用以下公式来表示：</p>\n<h3 id=\"Encoder-Layer\">Encoder Layer</h3>\n<p><img src=\"/images/2021.05/transformer-encoder-layer.png\" alt=\"transformer\"></p>\n<p>其中N个Layer链式连接，上一个的输出就是下一个的输入（而且输入和输出的维度大小一致），通过层级连接，能够捕捉到不同层次之间的上下文关系，这个过程也可以认为是sentence-level并非word-level的建模。</p>\n<h4 id=\"Multi-Header-Layer\">Multi-Header Layer</h4>\n<p><img src=\"/images/2021.05/transformer-multi-head.png\" alt=\"transformer\"></p>\n<p>内部有h个Dot-Product Attention连接：多个独立可学习的参数的特征空间，提升某一层的宽度，提取更多的特征空间。</p>\n<blockquote>\n<p>有研究表明，深的网络比宽的网络效果更好。而Bert无论你是宽度还是深度都比较适中，或许这就是理想情况下的结构。</p>\n</blockquote>\n<ul>\n<li>multi-head：提取特征的数量</li>\n<li>N-Transformer：提取复杂的特征</li>\n</ul>\n<p>核心公式如下所示：</p>\n<p><img src=\"/images/2021.05/transformer-math-multi-head.png\" alt=\"transformer\"><br>\n<img src=\"/images/2021.05/transformer-match-dot-product.png\" alt=\"transformer-match-dot-product\"><br>\n<img src=\"/images/2021.05/transformer-attention.png\" alt=\"attention math\"></p>\n<blockquote>\n<p>QK^t 的结果大小是:(input_length * input_length)</p>\n</blockquote>\n<p><strong>可以将Attention看作是给sentence中不同token之间建模相似度关系</strong></p>\n<ul>\n<li>QK都是input通过不同的权重矩阵映射而来，最后通过dot-product计算映射的相似性，并最终得到一个token-level上的相似度。</li>\n<li>d_k 是放缩因子，能够减少不同维度大小带来的影响。</li>\n</ul>\n<h4 id=\"SEP-token\">[SEP] token</h4>\n<p>Bert 其实是可以应用于多种下有任务当中，其中有一个应用场景就是需要对两段文本进行编码，比如Question Answer任务当中就需要将两个句子拼接到一起，然后塞入到Bert当中编码。那对两个句子进行语义层面的分割便成为了一个问题。</p>\n<p>于是就引入了[SEP]标签：在每一个文本的末尾都添加一个[SEP]标签来分割两端，从而让模型</p>\n<h4 id=\"Dropout-Add-Norm\">Dropout, Add &amp; Norm</h4>\n<p>MultiHead 结构的输出是: (input_length, embedding_dim)，接着将会有Dropout、残差网络已经正则化的处理过程，两处的Dropout都是0.1。</p>\n<p>SubLayer：FeedForward、Multi-Head。整体公式为：x + Dropout(Sublayer(x))。</p>\n<p>接着会有一个token-wise/row-wise级别的正则化来保证该层数据的稳定性，以此来保证参数学习的范围不要太偏。</p>\n<p><img src=\"/images/2021.05/transformer-layer.png\" alt=\"upload successful\"></p>\n<h4 id=\"理解MultiHead\">理解MultiHead</h4>\n<p>整体理解：Token Representation + Token Relationship Representation。</p>\n<p>换句话说：保留原始token的语义，理解强相关的词与词之间的关系。</p>\n<h4 id=\"总结\">总结</h4>\n<ul>\n<li>Bert仅仅是使用了Transformer中的Encoder。</li>\n<li>由于每一层的输入和输出维度大小一致，故是可以使用链式连接建模。</li>\n</ul>\n<h2 id=\"训练方法\">训练方法</h2>\n<p>Bert是双向自编码语言模型，训练阶段使用了两种方法：Mask Language Model 以及 Next Sentence Prediction。前者主要是为了学习一个丰富的上下文信息，后者为部分下游任务设计（如QA、NLI等任务），目的是建模句子与句子之间的联系。</p>\n<h2 id=\"一堆问题\">一堆问题</h2>\n<h3 id=\"Self-Attention-is-BiDirectional\">Self-Attention is BiDirectional</h3>\n<blockquote>\n<p>参考:</p>\n<ul>\n<li><a href=\"https://github.com/google-research/bert/issues/83\">The feature of bidirection</a></li>\n<li><a href=\"https://github.com/google-research/bert/issues/319#issuecomment-466844140\">how the model reflect ‘bidirectional’?</a></li>\n</ul>\n</blockquote>\n<ul>\n<li>Bert是双向模型，基于Transformer Encoder中的 Bidirectional Self-Attention。\n<ul>\n<li>Self-Attention 可以获取left-tokens 和 right-tokens，所以被称为是bidirectional。</li>\n</ul>\n</li>\n<li>GPT 是单向模型，基于Transformer Decoder，仅仅是将每一个是token都添加为输入，并生成对应的输出。\n<ul>\n<li>只能够将left-tokens添加到输入当中，输出预测tokens，故称之为unidirectional。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"为什么-CLS-能作为整个文本的表示\">为什么 [CLS] 能作为整个文本的表示</h3>\n<blockquote>\n<p>参考: <a href=\"https://github.com/google-research/bert/issues/319#issuecomment-522072295\">why is cls learning the sentence representation?</a></p>\n</blockquote>\n<p>Bert的输入中，每一个文本开头都会插入一个[CLS]标签，在文本分类的过程中会使用这个位置的标签来作为整个sentence的representation，实践证明效果是非常好的，可是为什么呢？</p>\n<ol>\n<li>这个要从Self-Attention说起。Self-Attention中的每一个tokne都可以获得整个sentence的tokens表示，也就是全局的含义表示。而[CLS]虽然是在文本的第一个位置，可是依然不影响能够获得全局的编码。</li>\n<li>选择固定位置的token和指定位置的token表示，其实效果差别挺大的。比如在文本中：<code>the cat in the hat</code>和<code>i like the cat</code>中的<code>the</code>表示的含义是不一致的，而且位置信息也不一致的，没有办法通过指定的token来表示整个sentence。所以选择一个固定位置的token来作为sentence的表示显得尤为重要。</li>\n<li>[CLS]标签在第一层Transformer中只是一个初始化的token embedding表示，可是在Self-Attention的加持下，在每一层Transformer迭代的时候，都能够添加获不同层面的all token 编码信息，所以在最后一层的时候是有丰富的sentence-level信息编码。</li>\n</ol>\n<h2 id=\"总结-2\">总结</h2>\n<ul>\n<li>[MASK]标记在实际预测中不会出现，训练时用过多[MASK]影响模型表现</li>\n<li>每个batch只有15%的token被预测，所以BERT收敛得比left-to-right模型要慢（它们会预测每个token）</li>\n</ul>\n<h1>衍生模型</h1>\n<h2 id=\"RoBERTa\">RoBERTa</h2>\n<h2 id=\"XLNet\">XLNet</h2>\n<h2 id=\"AlBERTa\">AlBERTa</h2>\n<h2 id=\"FastBert\">FastBert</h2>\n<p>权重共享的BERT。</p>\n<hr>\n<p>参考链接：</p>\n<ul>\n<li><a href=\"https://medium.com/@mromerocalvo/dissecting-bert-part1-6dcf5360b07f\">introduction-for-bert-part-1</a></li>\n<li><a href=\"https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73\">introduction-for-bert-part-2</a></li>\n<li><a href=\"https://towardsdatascience.com/intuitive-explanation-of-bert-bidirectional-transformers-for-nlp-cdc1efc69c1e\">Intuitive Explanation of BERT- Bidirectional Transformers for NLP</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/46652512\">知识点扩展</a></li>\n</ul>\n","categories":["nlp","pretrained-language-model"],"tags":["nlp","bert"]},{"title":"about","url":"https://wj-mcat.github.io/about/index.html","content":"<p>Hello, developers.</p>\n<p>I’m 吴京京 👨‍💻, author of <a href=\"https://github.com/wechaty/python-wechaty\">python-wechaty</a>, intern of <a href=\"http://github.com/microsoft/\">microsoft</a>, artificial intelligence master of <a href=\"https://www.bupt.edu.cn/\">BUPT</a>, great passion about Chatbot.</p>\n<p>🎃 🎃 Latest activites:</p>\n<ul>\n<li>📄 writing documentation system of <a href=\"https://python-wechaty.readthedocs.io/\">python-wechaty</a></li>\n<li>📖 writing books: 《<a href=\"https://github.com/wechaty/chatbot-1-to-2\">Chatbot 从1到2</a>》</li>\n<li>👐 give more energy on <a href=\"https://github.com/wechaty/wechaty\">wechaty</a></li>\n<li>🦈 …</li>\n</ul>\n<p>Talks:</p>\n<ul>\n<li>2021.4.14 <a href=\"https://www.bilibili.com/video/BV16U4y1h7dc\">几行代码带你打造专属的AI ChatBot</a></li>\n<li>2020.11.21 <a href=\"https://ng-china.org/#speakers\">基于机器学习的聊天机器人</a></li>\n<li>2020.11.14 <a href=\"https://wx.vzan.com/live/tvchat-425619793#/\">开源教育：2:41:24 Multi-Wechaty SDK for Chatbot</a></li>\n<li>2020.10.24 <a href=\"https://segmentfault.com/area/coscon-2020\">2020 中国开源年会暨阿帕奇中国路演 - 人工智能OSS + AI | 分会场 - Multi：适合聊天机器人的Wechaty SDK</a></li>\n</ul>\n","categories":[],"tags":[]}]